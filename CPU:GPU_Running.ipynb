{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8PilO4Tpt4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7865cd6f-3ed5-4db3-f77b-f370711fc81a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#import library\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import json\n",
        "#mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47O34NgOja1J"
      },
      "outputs": [],
      "source": [
        "# Revise the FlatDataflow function to make it clearer\n",
        "#!nvidia-smi\n",
        "\n",
        "def FlatDataflow(loop_num, query, key, value, bias, batch_granularity_level=1, head_granularity_level=8, length_granularity_level=64, batchTrue=False, headTrue=True, lengthTrue=False):\n",
        "  if (tf.config.list_physical_devices('GPU')):\n",
        "    memory_before = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Iteration %d: Before running, reset the memory! Memory peak: %f; Memory current: %f\"%(loop_num, memory_before['peak'], memory_before['current']))\n",
        "    ##If the dimension of the input tensor is 3 rather than 4, expand its dimension for the batch\n",
        "    if (len(query.shape) == 3):\n",
        "      query = query[None, :, :, :]\n",
        "      key = key[None, :, :, :]\n",
        "      value = value[None, :, :, :]  \n",
        "    batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "    # Set a fixed bias value here\n",
        "    bias_value = bias\n",
        "    for batch in tf.range(0, batch_size, batch_granularity_level):\n",
        "      # The lowest granularity is batch level.\n",
        "      if (batch_granularity_level != 1 or batchTrue):\n",
        "        end_batch = batch + batch_granularity_level if batch + batch_granularity_level <= batch_size else batch_size\n",
        "        query_source = tf.gather(query[:, :, :, :], indices=tf.range(batch, end_batch), axis=0)\n",
        "        key_source = tf.gather(key[:, :, :, :], indices=tf.range(batch, end_batch), axis=0)\n",
        "        value_source = tf.gather(value[:, :, :, :], indices=tf.range(batch, end_batch), axis=0)\n",
        "        result = tf.einsum(\"BTNH, BFNH->BNFT\", key_source, query_source)\n",
        "        result += bias_value\n",
        "        result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "        result = tf.nn.dropout(result, rate=0.4)\n",
        "        attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", result, value_source)\n",
        "      else:\n",
        "        for head in tf.range(0, head_num, head_granularity_level):\n",
        "          # The lowest granularity is head level.\n",
        "          if (head_granularity_level != 1 or headTrue):\n",
        "            end_head = head + head_granularity_level if head + head_granularity_level <= head_num else head_num\n",
        "            query_source = tf.gather(query[batch, :, :, :], indices=tf.range(head, end_head), axis=1)\n",
        "            key_source = tf.gather(key[batch, :, :, :], indices=tf.range(head, end_head), axis=1)\n",
        "            value_source = tf.gather(value[batch, :, :, :], indices=tf.range(head, end_head), axis=1)\n",
        "            result = tf.einsum(\"TNH, FNH->NFT\", key_source, query_source)\n",
        "            result += bias_value\n",
        "            result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "            result = tf.nn.dropout(result, rate=0.4)\n",
        "            logit = tf.einsum(\"NFT,TNH->FNH\", result, value_source)\n",
        "            if head == 0:\n",
        "              attention_output = logit\n",
        "            else:\n",
        "              attention_output = tf.concat([attention_output, logit], axis=1)\n",
        "          else:\n",
        "            #Lowest granularity is length level\n",
        "            for length in tf.range(0, source_length, length_granularity_level):\n",
        "              end_length = length + length_granularity_level if length + length_granularity_level <= source_length else source_length\n",
        "              query_source = tf.gather(query[batch, :, head, :], indices=tf.range(length, end_length), axis=0)\n",
        "              key_source = key[batch, :, head, :]\n",
        "              result = tf.einsum(\"TH, FH->FT\", key_source, query_source)\n",
        "              result += bias_value\n",
        "              result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "              result = tf.nn.dropout(result, rate=0.4)\n",
        "              if length == 0:\n",
        "                lengthOutput = result\n",
        "              else:\n",
        "                lengthOutput = tf.concat([lengthOutput, result], axis=0)\n",
        "            value_source = value[batch, :, head, :]\n",
        "            lengthRes = tf.einsum(\"FT,TH->FH\", lengthOutput, value_source)\n",
        "            lengthRes = tf.expand_dims(lengthRes, axis=1)\n",
        "            if (head == 0):\n",
        "              attention_output = lengthRes\n",
        "            else:\n",
        "              attention_output = tf.concat([attention_output, lengthRes], axis=1)\n",
        "        attention_output = tf.expand_dims(attention_output, axis=0)\n",
        "      if (batch == 0):\n",
        "        output = attention_output\n",
        "      else:\n",
        "        output = tf.concat([output, attention_output], axis=0)\n",
        "    stoptime = time.time()\n",
        "    # Clear out the memory\n",
        "    print(\"LOOP\")\n",
        "    del output\n",
        "    del attention_output\n",
        "    memory_after = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(\"Iteration %d: After running! Memory peak: %f; Memory current: %f\"%(loop_num, memory_after['peak'], memory_after['current']))\n",
        "    #return (memory_before['peak'], memory_before['current'], memory_after['peak'], memory_after['current'], stoptime)\n",
        "    return (0, 0, 0, 0, stoptime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MksvcgKAffO"
      },
      "outputs": [],
      "source": [
        "# Code block building graph with x-axis as sequence length and y-axis as peak memory usage\n",
        "\n",
        "# Read in all the files\n",
        "query_file = glob.glob(\"/content/drive/MyDrive/models/transformer/logging_query*.txt\")\n",
        "key_file = glob.glob(\"/content/drive/MyDrive/models/transformer/logging_key*.txt\")\n",
        "value_file = glob.glob(\"/content/drive/MyDrive/models/transformer/logging_value*.txt\")\n",
        "\n",
        "# Randomly set a bias value for now\n",
        "BIAS = 0.02\n",
        "\n",
        "peak = []\n",
        "curr = []\n",
        "running_time = []\n",
        "\n",
        "#tf.config.experimental.reset_memory_stats('GPU:0')\n",
        "FILENUM = len(query_file)\n",
        "BATCHSIZE = 64\n",
        "\n",
        "queryin = []\n",
        "keyin = []\n",
        "valuein = []\n",
        "for file in query_file:\n",
        "    qfile = tf.io.read_file(file)\n",
        "    query = tf.io.parse_tensor(qfile, out_type=tf.float32)\n",
        "    queryin.append(query)\n",
        "for file in key_file:\n",
        "    kfile = tf.io.read_file(file)\n",
        "    key = tf.io.parse_tensor(kfile, out_type=tf.float32)\n",
        "    keyin.append(key)\n",
        "for file in value_file:\n",
        "    vfile = tf.io.read_file(file)\n",
        "    value = tf.io.parse_tensor(vfile, out_type=tf.float32)\n",
        "    valuein.append(value)\n",
        "queryin = tf.stack(queryin)\n",
        "keyin = tf.stack(keyin)\n",
        "valuein = tf.stack(valuein)\n",
        "\n",
        "# Set up the parameters\n",
        "batch = 1\n",
        "head = 4\n",
        "length = 1024\n",
        "batchTrue = False\n",
        "headTrue = True\n",
        "lengthTrue = False\n",
        "\n",
        "fileidx = np.random.randint(FILENUM)\n",
        "batchidx = np.random.randint(BATCHSIZE)\n",
        "query = queryin[fileidx][batchidx, :, :, :]\n",
        "key = keyin[fileidx][batchidx, :, :, :]\n",
        "value = valuein[fileidx][batchidx, :, :, :]\n",
        "\n",
        "# Generate start matrix with shape 1 * 256 * 16 * 64\n",
        "# Randomly pick a file number\n",
        "for i in range(256 // 64 - 1):\n",
        "  fileidx = np.random.randint(FILENUM)\n",
        "  batchidx = np.random.randint(BATCHSIZE)\n",
        "  query = tf.concat((query, queryin[fileidx][batchidx, :, :, :]), axis=0)\n",
        "  key = tf.concat((key, keyin[fileidx][batchidx, :, :, :]), axis=0)\n",
        "  value = tf.concat((value, valuein[fileidx][batchidx, :, :, :]), axis=0)\n",
        "\n",
        "for idx in range(256 // 64, 12 * 1024 // 64, 1):\n",
        "  start_time = time.time()\n",
        "  peakOld, currOld, peakCurr, currCurr, stoptime = FlatDataflow(idx-256//64, query, key, value, BIAS, batch_granularity_level=batch, head_granularity_level=head, length_granularity_level=length,\n",
        "                                                                batchTrue=batchTrue, headTrue=headTrue, lengthTrue=lengthTrue)\n",
        "  peak.append(peakCurr)\n",
        "  curr.append(currCurr)\n",
        "  running_time.append(stoptime - start_time)\n",
        "  fileidx = np.random.randint(FILENUM)\n",
        "  batchidx = np.random.randint(BATCHSIZE)\n",
        "\n",
        "  query = tf.concat((query, queryin[fileidx][batchidx, :, :, :]), axis=0)\n",
        "  key = tf.concat((key, keyin[fileidx][batchidx, :, :, :]), axis=0)\n",
        "  value = tf.concat((value, valuein[fileidx][batchidx, :, :, :]), axis=0)\n",
        "iteration = np.arange(len(peak)) * 64 + 256\n",
        "plt.plot(iteration, peak,'r',label=\"Peak\")\n",
        "plt.plot(iteration,curr,'b',label='Current')\n",
        "plt.legend()\n",
        "plt.title(\"Peak Memory Usage\")\n",
        "plt.xlabel(\"Source Length\")\n",
        "plt.ylabel(\"Bytes in usage\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "FinalDemo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}