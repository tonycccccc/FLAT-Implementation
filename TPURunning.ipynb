{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAH-ijAgAFlr"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_JOSIdKBTvr"
      },
      "outputs": [],
      "source": [
        "# Initialize TPU\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1Lsl78YxuO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ab71bd-158c-4b7a-c1bb-746565a80946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022/03/03 04:52:51.588918 Start gcsfuse/0.40.0 (Go version go1.17.6) for app \"\" using mount point: /content\n",
            "2022/03/03 04:52:51.605851 Opening GCS connection...\n",
            "2022/03/03 04:52:51.815035 Mounting file system \"flatdataflow\"...\n",
            "2022/03/03 04:52:51.853941 File system has been successfully mounted.\n"
          ]
        }
      ],
      "source": [
        "# Mount google cloud storage bucket\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "!gcsfuse --implicit-dirs flatdataflow /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z8M4Ur_As2g"
      },
      "outputs": [],
      "source": [
        "# Incorporate the platform feature\n",
        "def FlatDataflow(loop_num, query, key, value, bias, batch_granularity_level=1, head_granularity_level=8, length_granularity_level=64, running_platform='CPU'):\n",
        "  # If the dimension of the input tensor is 3 rather than 4, expand its dimension for the batch\n",
        "  if (len(query.shape) == 3):\n",
        "    query = query[None, :, :, :]\n",
        "    key = key[None, :, :, :]\n",
        "    value = value[None, :, :, :]  \n",
        "  batch_size, source_length, head_num, dim = tf.shape(query).numpy()\n",
        "  # Set a fixed bias value here\n",
        "  bias_value = bias\n",
        "  for batch in tf.range(0, batch_size, batch_granularity_level):\n",
        "    # The lowest granularity is batch level.\n",
        "    if (batch_granularity_level != 1):\n",
        "      end_batch = batch + batch_granularity_level if batch + batch_granularity_level <= batch_size else batch_size\n",
        "      query_source = tf.gather(query[:, :, :, :], indices=tf.range(batch, end_batch), axis=0)\n",
        "      key_source = tf.gather(key[:, :, :, :], indices=tf.range(batch, end_batch), axis=0)\n",
        "      value_source = tf.gather(value[:, :, :, :], indices=tf.range(batch, end_batch), axis=0)\n",
        "      result = tf.einsum(\"BTNH, BFNH->BNFT\", key_source, query_source)\n",
        "      result += bias_value\n",
        "      result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "      result = tf.nn.dropout(result, rate=0.4)\n",
        "      attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", result, value_source)\n",
        "    else:\n",
        "      for head in tf.range(0, head_num, head_granularity_level):\n",
        "        # The lowest granularity is head level.\n",
        "        if (head_granularity_level != 1):\n",
        "          end_head = head + head_granularity_level if head + head_granularity_level <= head_num else head_num\n",
        "          query_source = tf.gather(query[batch, :, :, :], indices=tf.range(head, end_head), axis=1)\n",
        "          key_source = tf.gather(key[batch, :, :, :], indices=tf.range(head, end_head), axis=1)\n",
        "          value_source = tf.gather(value[batch, :, :, :], indices=tf.range(head, end_head), axis=1)\n",
        "          result = tf.einsum(\"TNH, FNH->NFT\", key_source, query_source)\n",
        "          result += bias_value\n",
        "          result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "          result = tf.nn.dropout(result, rate=0.4)\n",
        "          logit = tf.einsum(\"NFT,TNH->FNH\", result, value_source)\n",
        "          if head == 0:\n",
        "            attention_output = logit\n",
        "          else:\n",
        "            attention_output = tf.concat([attention_output, logit], axis=1)\n",
        "        else:\n",
        "          #Lowest granularity is length level\n",
        "          for length in tf.range(0, source_length, length_granularity_level):\n",
        "            end_length = length + length_granularity_level if length + length_granularity_level <= source_length else source_length\n",
        "            query_source = tf.gather(query[batch, :, head, :], indices=tf.range(length, end_length), axis=0)\n",
        "            key_source = key[batch, :, head, :]\n",
        "            result = tf.einsum(\"TH, FH->FT\", key_source, query_source)\n",
        "            result += bias_value\n",
        "            result = tf.nn.softmax(result, name=\"attention_weights\")\n",
        "            result = tf.nn.dropout(result, rate=0.4)\n",
        "            if length == 0:\n",
        "              lengthOutput = result\n",
        "            else:\n",
        "              lengthOutput = tf.concat([lengthOutput, result], axis=0)\n",
        "          value_source = value[batch, :, head, :]\n",
        "          lengthRes = tf.einsum(\"FT,TH->FH\", lengthOutput, value_source)\n",
        "          lengthRes = tf.expand_dims(lengthRes, axis=1)\n",
        "          if (head == 0):\n",
        "            attention_output = lengthRes\n",
        "          else:\n",
        "            attention_output = tf.concat([attention_output, lengthRes], axis=1)\n",
        "      attention_output = tf.expand_dims(attention_output, axis=0)\n",
        "    if (batch == 0):\n",
        "      output = attention_output\n",
        "    else:\n",
        "      output = tf.concat([output, attention_output], axis=0)\n",
        "  stoptime = time.time()\n",
        "  return stoptime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHPTiYCe1mIK"
      },
      "outputs": [],
      "source": [
        "# Readin data\n",
        "queryin = []\n",
        "keyin = []\n",
        "valuein = []\n",
        "for idx in range(3, 12):\n",
        "  query_path = \"/content/FILES/logging_query\"+str(idx)+\".txt\"   \n",
        "  with open(query_path, \"rb\") as query:\n",
        "    query_file = query.read()\n",
        "  query = tf.io.parse_tensor(query_file, out_type=tf.float32)\n",
        "  queryin.append(query)\n",
        "  key_path = \"/content/FILES/logging_key\"+str(idx)+\".txt\"   \n",
        "  with open(key_path, \"rb\") as key:\n",
        "    key_file = key.read()\n",
        "  key = tf.io.parse_tensor(key_file, out_type=tf.float32)\n",
        "  keyin.append(key)\n",
        "  value_path = \"/content/FILES/logging_value\"+str(idx)+\".txt\"   \n",
        "  with open(value_path, \"rb\") as value:\n",
        "    value_file = value.read()\n",
        "  value = tf.io.parse_tensor(value_file, out_type=tf.float32)\n",
        "  valuein.append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9ePuPngE62d"
      },
      "outputs": [],
      "source": [
        "# Randomly set a bias value for now\n",
        "BIAS = 0.02\n",
        "\n",
        "running_time = []\n",
        "FILENUM = 9\n",
        "BATCHSIZE = 64\n",
        "queryin = tf.stack(queryin)\n",
        "keyin = tf.stack(keyin)\n",
        "valuein = tf.stack(valuein)\n",
        "print(\"Files Successfully Loaded!\")\n",
        "\n",
        "# Set up the parameters\n",
        "batch = 1\n",
        "head = 4\n",
        "length = 1\n",
        "\n",
        "fileidx = np.random.randint(FILENUM)\n",
        "batchidx = np.random.randint(BATCHSIZE)\n",
        "query = queryin[fileidx][batchidx, :, :, :]\n",
        "key = keyin[fileidx][batchidx, :, :, :]\n",
        "value = valuein[fileidx][batchidx, :, :, :]\n",
        "\n",
        "# Generate start matrix with shape 1 * 256 * 16 * 64\n",
        "# Randomly pick a file number\n",
        "for i in range(256 // 64 - 1):\n",
        "  fileidx = np.random.randint(FILENUM)\n",
        "  batchidx = np.random.randint(BATCHSIZE)\n",
        "  query = tf.concat((query, queryin[fileidx][batchidx, :, :, :]), axis=-3)\n",
        "  key = tf.concat((key, keyin[fileidx][batchidx, :, :, :]), axis=-3)\n",
        "  value = tf.concat((value, valuein[fileidx][batchidx, :, :, :]), axis=-3)\n",
        "\n",
        "for idx in range(256 // 64, 16 * 1024 // 64, 1):\n",
        "  start_time = time.time()\n",
        "  stoptime = FlatDataflow(idx-256//64, query, key, value, BIAS, batch_granularity_level=batch, head_granularity_level=head, length_granularity_level=length)\n",
        "  running_time.append(stoptime - start_time)\n",
        "  fileidx = np.random.randint(FILENUM)\n",
        "  batchidx = np.random.randint(BATCHSIZE)\n",
        "\n",
        "  query = tf.concat((query, queryin[fileidx][batchidx, :, :, :]), axis=0)\n",
        "  key = tf.concat((key, keyin[fileidx][batchidx, :, :, :]), axis=0)\n",
        "  value = tf.concat((value, valuein[fileidx][batchidx, :, :, :]), axis=0)\n",
        "  print(\"LOOP %d\" % (idx) )\n",
        "\n",
        "print(\"Finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code block --TO BE FIXED\n",
        "dataset_list = []\n",
        "for idx in range(3, 12):\n",
        "  query_path = \"/content/FILES/logging_query\"+str(idx)+\".txt\"   \n",
        "  with open(query_path, \"rb\") as query:\n",
        "    query_file = query.read()\n",
        "  query = tf.io.parse_tensor(query_file, out_type=tf.float32)\n",
        "  key_path = \"/content/FILES/logging_key\"+str(idx)+\".txt\"   \n",
        "  with open(key_path, \"rb\") as key:\n",
        "    key_file = key.read()\n",
        "  key = tf.io.parse_tensor(key_file, out_type=tf.float32)\n",
        "  value_path = \"/content/FILES/logging_value\"+str(idx)+\".txt\"   \n",
        "  with open(value_path, \"rb\") as value:\n",
        "    value_file = value.read()\n",
        "  value = tf.io.parse_tensor(value_file, out_type=tf.float32)\n",
        "  # Add each (query, key, value) tuple to the list\n",
        "  dataset_list.append((query, key, value))\n",
        "\n",
        "#Form the tf dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(dataset_list)\n",
        "\n",
        "#Distribute training code\n",
        "strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n",
        "dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "\n",
        "@tf.function\n",
        "def replica_fn(input):\n",
        "  #Decouple the tuple\n",
        "  query, key, value = input\n",
        "  stoptime = FlatDataflow(query, key, value)\n",
        "  return stoptime\n",
        "\n",
        "result = []\n",
        "# Iterate over the `tf.distribute.DistributedDataset` for distribute running\n",
        "for x in dist_dataset:\n",
        "  # process dataset elements\n",
        "  result.append(strategy.run(replica_fn, args=(x,)))"
      ],
      "metadata": {
        "id": "a8LPfFUcNZHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "TPURunning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}